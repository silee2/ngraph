What is represented by GFX?
Roughly mathematical equations and functions expressed by flow graph of primitives.
Primitives maybe type insensitive but some are tied to a catagory of types (For example, boolean).
Different level of device independent abstractions can be represented with the following.
Analysis, transformation and optimization can happen at different level of abstraction.
Level 0: Unbounded primitives with static attributes
Level 1: Input Tensor shape and element type (not for every node) can be applied to make the flow graph more specific.
Still there is no concept of variables or memory yet and targeting an abbstract device.
Level 2: Shape and element types are inferred for all nodes (if they are not value dependent).
Device specific abstraction can branch off from any of the above levels.



Device
Definition: abstraction for a entity that has storage and compute capability
A default device must exist.
Every device must provide a to_host method that moves value from device to host and a from_host method that moves a value from host to device.


Value
Definition: an entity that is device independent
Value is opaque and can only be accessed with to_plain method
This enables lazy evaluation as Value can actually be a built up as a higher order function that produces an entity.
Value is wrapper that is applied to node and binds inputs to the node.
Value : Tensor
Value : (Value)* -> Node

Node
Definition: node is an entity that can have zero or more runtime inputs and produces 1 or 2 runtime output (that may feed in to many nodes.)
First output is a special output (Output 0) that represents signal for state change.
Node are like math functions except that zero input is allowed and has a special output in addition to zero or more output.

Node can have static attributes.
Node does not need to bind inputs or outputs at construction time. Static atrributes need to be given at construction.
Every node must provide an eval method, given ordered host input value(s), produces host output value.
Devices can provide an eval method for nodes.
Every node must provide an memory_estimate() and compute_estimate() method
Every node must provide an infer output shape method
Every node must provide an forward method

"Nodes are device independent"
"Values live on device"
"Graph describes computation(flow) in an abstract way."


Output (def-use chain): captures a single output value from a node and connects it to input(s) of one or more nodes.

Input (use-def chain): captures a single input value that is produced from a node.

Op: Op is a collection of connected nodes (or pattern of nodes)
Op has inputs and output(s)
Op must provide an ordered mapping of its inputs and outputs to internal node's inputs and outputs.
Op is a graph builder pattern
Device may provide a specialized eval method for an Op. If not, a fallback that calls eval methods of internal nodes is provided.


Op(Pattern) Matching:

Graph execution:
- Define as go (eager)
- Frozen graph
- Asynchronous
- Lazy
- Multi device
- Hetro device

Shape:
How does output shape of value change?
Padding - changes dim by counts (add subtract)
Windowed operation - Decides rank and dim through output iterator
Reduction - - Changes dim by factors (multiply)
Contraction - Inner product, changes rank
Select - Changes rank and dim

How does input shape of value change?
Auto broadcasting - tensor data does not need to change. rank, dim and stride needs to be updated.

Shape inference:
1. input shape inference: needed for auto broadcasting (done by either stride/rank update or physical broadcast).
2. output shape inference: needed for output (device) memory allocation.

Eager Evaluation(Simple Interpreter):
Simple kernels - Does not handle complex stride, physical broadcast is needed, blocked format is not supported.
Input shape infererence, output shape inference, dispatch kernel.

Lazy Evaluation
Input shape inference, output shape inference, forward futures.
Futures eval (explicit read/write request) triggers memory allocation and actual execution.

Device memory allocation
